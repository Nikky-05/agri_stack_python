# AgriStack MIS Analytics - Professional Modular Platform

A high-performance, conversational analytics dashboard for the AgriStack ecosystem. This platform allows government officials to query agricultural data using natural language, visualizing results through interactive charts and premium KPI indicators.

## ğŸ—ï¸ Modular Architecture

The project is structured following professional software engineering principles:

- **`app.py`**: The central FastAPI orchestrator. Handles routing and coordinates between logic handlers.
- **`config.py`**: Centralized configuration management (DB credentials, LGD mappings, system constants).
- **`nlp_handler.py`**: The intelligence layer. Responsible for **Intent Classification** (understanding what the user wants) and **Natural Language Generation** (narrating data results).
- **`db_handler.py`**: The SQL engine. Manages all interactions with the PostgreSQL database.
- **`csv_handler.py`**: The fallback/flat-file engine. Allows the system to run directly from local CSV files using Pandas for maximum flexibility.
- **`setup_db.py`**: Data migration utility. Imports raw CSV data into the structured PostgreSQL schema.

## ğŸš€ Deployment & Setup

### 1. Environment Configuration
Update the `.env` file or modify `config.py` with your database credentials and environment-specific settings.

### 2. Database Migration
To import the raw agricultural data into your PostgreSQL instance:
```bash
python setup_db.py
```

### 3. Running the Platform
Launch the backend server:
```bash
python app.py
```
The API will be available at `http://localhost:8001`.

### 4. Frontend Access
Open `frontend/index.html` in any modern browser to access the interactive dashboard.

## ğŸ“Š Feature Highlights
- **Smart Data Toggle**: Switch between Database and CSV mode instantly in `app.py`.
- **LGD-Based Authorization**: Built-in security to ensure users only access data within their permitted LGD scope.
- **Interactive Visuals**: Real-time generation of Bar, Pie, and Line charts using Chart.js.
- **Premium UX**: Responsive layout with glassmorphism and state-of-the-art animations.




âœ» Conversation compacted (ctrl+o for history)

  â¿  Read requirements.txt (24 lines)
  â¿  Read nlp_handler.py (435 lines)
  â¿  Read config.py (282 lines)
  â¿  Referenced file frontend\style.css
  â¿  Referenced file frontend\script.js

â— Looking at your code, the Ollama integration is correctly set up. Let me verify the key components:

  Ollama Integration in nlp_handler.py:                                                                                        - âœ… Connects to http://localhost:11434/api/generate
  - âœ… Default model: llama3:latest                                                                                          
  - âœ… Model verification via /api/tags endpoint
  - âœ… Timeout handling (15-20 seconds)
  - âœ… Fallback to template narration if LLM fails

  Dependencies Check:
  The requests library is included in requirements.txt for HTTP calls to Ollama.

  To ensure Ollama works properly, you need:

  1. Ollama installed and running - Start it with:
  ollama serve
  2. llama3 model downloaded - If not already:
  ollama pull llama3
  3. Verify it's running:
  curl http://localhost:11434/api/tags

  Your code handles Ollama gracefully:
  - If Ollama is not running â†’ Falls back to template-based responses
  - If model doesn't exist â†’ Tries llama3:latest as fallback
  - If LLM response is too short/long â†’ Uses template narration

  Summary of your complete stack:
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚         Component         â”‚              Status               â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚ FastAPI backend           â”‚ âœ… Ready                          â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚ NLP intent classification â”‚ âœ… Fixed (cultivated vs surveyed) â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚ CSV data processing       â”‚ âœ… Working                        â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚ PostgreSQL query format   â”‚ âœ… Correct with is_view=true      â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚ Ollama LLM narration      â”‚ âœ… Configured (optional)          â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚ Frontend UI               â”‚ âœ… Cinematic 3D design            â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚ requirements.txt          â”‚ âœ… Updated                        â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  Your application will work even without Ollama running - it will just use the template-based responses instead of
  LLM-generated narrations. Everything is good to go.